# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.axes as ax

# %%
data = pd.read_csv('data_for_lr.csv')

# %%
data = data.dropna()
print("Shape of the dataset = {}".format(data.shape))

# %%
data.head()

# %%
# training dataset and labels
train_input = np.array(data.x[0:500]).reshape(500,1)
train_output  = np.array(data.y[0:500]).reshape(500,1)

# valid dataset and labels
test_input = np.array(data.x[500:700]).reshape(199,1)
test_output  = np.array(data.y[500:700]).reshape(199,1)

# print the shapes
print("Train Input Shape = {}".format(train_output.shape))
print("Train Output  Shape = {}".format(train_output.shape))
print("Test Input Shape = {}".format(test_input.shape))
print("Test Output  Shape = {}".format(test_output.shape))

# %%
def forward_propagation(train_input, parameters):
    m = parameters['m']
    c = parameters['c']
    predictions = np.multiply(m, train_input) + c
    return predictions

# %%
def cost_function(predictions, train_output):
    cost = np.mean((train_output - predictions) ** 2) * 0.5
    return cost

# %%
def backward_propagation(train_input, train_output, predictions):
    derivatives = dict()
    df = (train_output - predictions) * -1
    dm = np.mean(np.multiply(train_input, df))
    dc = np.mean(df)
    derivatives['dm'] = dm
    derivatives['dc'] = dc
    return derivatives

# %%
def update_parameters(parameters, derivatives, learning_rate):
    parameters['m'] = parameters['m'] - learning_rate * derivatives['dm']
    parameters['c'] = parameters['c'] - learning_rate * derivatives['dc']
    return parameters

# %%
def train(train_input, train_output, learning_rate, iters):
    
    #random parameters
    parameters = dict()
    parameters["m"] = np.random.uniform(0,1) 
    parameters["c"] = np.random.uniform(0,1)
    
    plt.figure()
    
    #loss
    loss = list()
    
    #iterate
    for i in range(iters):
        
        #forward propagation
        predictions = forward_propagation(train_input, parameters)
        
        #cost function
        cost = cost_function(predictions, train_output)
        
        #append loss and print
        loss.append(cost)
        print("Iteration = {}, Loss = {}".format(i+1, cost))
        
        #plot function
        fig, ax = plt.subplots()
        
        ax.plot(train_input, train_output, '+', label='Original')
        ax.plot(train_input, predictions, '*', label='Training')

        legend = ax.legend()
        
        plt.plot(train_input, train_output, '+')
        plt.plot(train_input, predictions, '*')
        plt.show()
        
        #back propagation
        derivatives = backward_propagation(train_input, train_output, predictions)
        
        #update parameters
        parameters = update_parameters(parameters, derivatives, learning_rate)
        
    return parameters, loss

# %% [markdown]
# ## Training

# %%
parameters, loss = train(train_input, train_output, 0.0001, 20)

# %%
## plotting loss or gradinet descent function
fig=plt.axes()
fig.plot(loss,"--")
fig.set(xlabel="iterations",ylabel="loss",title='Gradient Descent')
fig.plot()

# %%
test_predictions = test_input * parameters["m"] + parameters["c"]
plt.figure()
plt.plot(test_input, test_output, '+')
plt.plot(test_input, test_predictions, '.')
plt.show()

# %%


# %%
plt.plot(parameters,loss,"*",color="green")
plt.show()

# %%

Experiment - 5 
 
AIM: The aim of this study is to implement Logistic Regression for binary classification and 
Softmax Regression for multiclass classification on different datasets, evaluating the 
performance of each model in predicting the target variables, such as survival in the Titanic 
dataset (binary) and species in the Iris dataset (multiclass). 
 
SOFTWARE REQUIRED: Google Colab 
 
THEORY: 
 
1. Logistic Regression (Binary Classification): 
Logistic regression is a statistical model used to predict the probability of a binary 
outcome (0 or 1). The model outputs a value between 0 and 1 using the sigmoid 
function. The logistic regression equation is: 
�
�(𝑌=1)= 1
 1+ⅇ−(𝛽0+𝛽1𝑋)
 
Where: 
• 𝑃(𝑌=1) is the probability of the target variable being 1. 
• 𝛽0 is the intercept. 
• 𝛽1 is the coefficient. 
• 𝑋 is the input feature(s). 
Logistic regression is widely used for binary classification tasks, such as predicting 
whether a passenger survived (Titanic dataset). 
 
2. Softmax Regression (Multiclass Classification): 
Softmax regression is an extension of logistic regression to handle multiclass 
classification problems. It uses the softmax function to predict the probabilities for 
each class in a classification task with more than two classes. For a set of 
classes C1,C2,...,CkC1,C2,...,Ck, the softmax function outputs the probability that a 
given input XX belongs to each class. 
The equation for the softmax function for class j is: 
�
�(𝐶𝑗|𝑥)= ⅇ𝛽𝑗
 𝑇𝑋
 ∑ ⅇ𝛽𝑘
 𝑇𝑋 𝐾
 𝑘=1
 
Where: 
• 𝑃(𝐶𝑗|𝑥)is the probability that input XX belongs to class 𝐶𝑗 
• 𝛽𝑗
 𝑇 are the coefficients associated with class 𝐶𝑗 
• K is the total number of classes. 
Softmax regression is useful for multiclass classification problems, such as classifying 
flower species in the Iris dataset. 
 
 
23 | Page 
 
3. Evaluation Metrics: 
• Accuracy: The proportion of correct predictions out of all predictions. 
• Precision: The ratio of true positives to the sum of true positives and false positives. 
• Recall: The ratio of true positives to the sum of true positives and false negatives. 
• F1-Score: The harmonic mean of precision and recall. 
 
CODE: 
A. IMPLEMENTATION OF LOGISTIC REGRESSION ON TITANIC DATASET 
 
Step 1: Importing required libraries and pre-processing the data 
 
RESULTS:  
• Logistic Regression (Titanic Dataset): 
Logistic regression provides an accuracy ranging from 78% to 82% for the Titanic dataset, 
depending on data preprocessing and feature engineering steps. The model’s performance 
can also be assessed using additional metrics like precision, recall, and F1-score, offering a 
comprehensive understanding of its effectiveness in predicting passenger survival. 
 
 
 
 
• Softmax Regression (Iris Dataset): 
On the Iris dataset, Softmax regression typically achieves high accuracy, often ranging 
between 90% and 98%, depending on the preprocessing. Softmax regression handles the 
three-class classification problem efficiently, offering robust performance across different 
feature sets. 
 
 
 
CONCLUSION:  
• Logistic Regression was effective for binary classification in predicting survival in the 
Titanic dataset, with a good balance of precision, recall, and F1-score. 
• Softmax Regression performed well for multiclass classification on the Iris dataset, with 
high accuracy and strong performance across all species. 
• Both models are suitable for their respective classification tasks, with Logistic Regression 
being ideal for binary classification and Softmax Regression excelling in multiclass 
classification scenarios.

