Experiment - 11 
 
AIM:  
The aim of this experiment is to compare and evaluate the performance of three different 
clustering algorithms—K-Means, Hierarchical Clustering, and DBSCAN—on a synthetic 
dataset. These methods are applied to group data points into clusters based on their features. 
The effectiveness of the clustering algorithms is assessed using the Silhouette Score, which 
measures how similar each point is to its own cluster compared to other clusters. The goal is to 
visualize the clustering results and determine which method performs best for this synthetic 
dataset. 
 
SOFTWARE REQUIRED: Google Colab 
 
THEORY: 
1. K-Means Clustering: 
o Concept: K-Means is a centroid-based clustering algorithm. It divides 
the data into K clusters by minimizing the sum of squared distances 
between data points and their corresponding cluster centroids. The 
algorithm iteratively assigns points to the nearest centroid and 
recalculates centroids until convergence. 
o Advantages: Fast, simple, and works well when clusters are 
spherical and roughly  equal in size. 
o Disadvantages: Sensitive to the initial placement of centroids and the value of 
K. 
2. Hierarchical Clustering: 
o Concept: Hierarchical clustering builds a tree-like structure of 
clusters. Agglomerative hierarchical clustering starts by treating each 
data point as its own cluster and then merges clusters iteratively based 
on their similarity. It can be visualized as a dendrogram. 
o Advantages: Does not require specifying the number of clusters in 
advance. The dendrogram provides a visual understanding of how 
clusters relate. 
o Disadvantages: Computationally expensive for large datasets. 
3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise): 
o Concept: DBSCAN clusters data points based on their density in the 
feature space. It groups closely packed points into clusters and labels 
points in low-density regions as outliers (noise). DBSCAN requires two 
parameters: eps (the radius of neighborhood around a point) and 
min_samples (minimum number of points required to form a cluster). 
o Advantages: Can find arbitrarily shaped clusters and does not require 
specifying the number of clusters. It can handle outliers well. 
o Disadvantages: Sensitive to the selection of parameters, especially eps and 
min_samples 
 
 
47 | Page 
 
 
CODE: 
 
Step 1: Generating the dataset and pre-processing the dataset 
 
 
Step 2: Plotting original dataset 
 
 
 
48 | Page 
 
Step 3: Applying different clustering methods 
 
 
 
 
 
 
 
49 | Page 
 
 
 
 
 
 
 
 
RESULTS: 
• K-Means Silhouette Score: 0.8447 
            The K-Means algorithm performs well on this synthetic dataset, achieving a high silhouette            
            score, indicating that the points are well-separated into distinct clusters. 
 
• Hierarchical Clustering Silhouette Score: 0.8447 
            Hierarchical clustering performs similarly to K-Means on this dataset, with the same silhouette    
50 | Page 
 
             score. This suggests that the structure of the dataset is well-suited for hierarchical methods, and              
             the clusters are similarly well-defined. 
 
• DBSCAN Silhouette Score: 0.8447 
            DBSCAN also achieves the same silhouette score, which indicates that the density- based    
            method also identifies well-separated clusters. However, it is worth noting that DBSCAN might    
            label some points as outliers (which might not be reflected in the silhouette score directly). 
 
 
CONCLUSION:  
• The K-Means, Hierarchical Clustering, and DBSCAN methods all perform well on 
this synthetic dataset with a high silhouette score, indicating good clustering 
performance. The results are consistent across all three methods, with similar 
silhouette scores. 
• DBSCAN has the advantage of identifying noise, which can be useful in datasets 
with outliers, while K-Means and Hierarchical Clustering are more deterministic 
and require a predefined number of clusters. 
• K-Means and Hierarchical Clustering are computationally more efficient and scale 
better with large datasets compared to DBSCAN, which can be sensitive to the 
selection of parameters. 
 
For datasets with clear, well-separated clusters, any of the three methods can perform well. 
However, for datasets with varying densities or noise, DBSCAN may provide better results.



# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt # for data visualization
import seaborn as sns # for statistical data visualization
%matplotlib inline

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.
/kaggle/input/facebook-live-sellers-in-thailand-uci-ml-repo/Live.csv
Ignore warnings
import warnings

warnings.filterwarnings('ignore')
7. Import dataset 
Table of Contents

data = '/kaggle/input/facebook-live-sellers-in-thailand-uci-ml-repo/Live.csv'

df = pd.read_csv(data)
8. Exploratory data analysis 
Table of Contents

Check shape of the dataset
df.shape
(7050, 16)
We can see that there are 7050 instances and 16 attributes in the dataset. In the dataset description, it is given that there are 7051 instances and 12 attributes in the dataset.

So, we can infer that the first instance is the row header and there are 4 extra attributes in the dataset. Next, we should take a look at the dataset to gain more insight about it.

Preview the dataset
df.head()
status_id	status_type	status_published	num_reactions	num_comments	num_shares	num_likes	num_loves	num_wows	num_hahas	num_sads	num_angrys	Column1	Column2	Column3	Column4
0	246675545449582_1649696485147474	video	4/22/2018 6:00	529	512	262	432	92	3	1	1	0	NaN	NaN	NaN	NaN
1	246675545449582_1649426988507757	photo	4/21/2018 22:45	150	0	0	150	0	0	0	0	0	NaN	NaN	NaN	NaN
2	246675545449582_1648730588577397	video	4/21/2018 6:17	227	236	57	204	21	1	1	0	0	NaN	NaN	NaN	NaN
3	246675545449582_1648576705259452	photo	4/21/2018 2:29	111	0	0	111	0	0	0	0	0	NaN	NaN	NaN	NaN
4	246675545449582_1645700502213739	photo	4/18/2018 3:22	213	0	0	204	9	0	0	0	0	NaN	NaN	NaN	NaN
View summary of dataset
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 7050 entries, 0 to 7049
Data columns (total 16 columns):
status_id           7050 non-null object
status_type         7050 non-null object
status_published    7050 non-null object
num_reactions       7050 non-null int64
num_comments        7050 non-null int64
num_shares          7050 non-null int64
num_likes           7050 non-null int64
num_loves           7050 non-null int64
num_wows            7050 non-null int64
num_hahas           7050 non-null int64
num_sads            7050 non-null int64
num_angrys          7050 non-null int64
Column1             0 non-null float64
Column2             0 non-null float64
Column3             0 non-null float64
Column4             0 non-null float64
dtypes: float64(4), int64(9), object(3)
memory usage: 881.4+ KB
Check for missing values in dataset
df.isnull().sum()
status_id              0
status_type            0
status_published       0
num_reactions          0
num_comments           0
num_shares             0
num_likes              0
num_loves              0
num_wows               0
num_hahas              0
num_sads               0
num_angrys             0
Column1             7050
Column2             7050
Column3             7050
Column4             7050
dtype: int64
We can see that there are 4 redundant columns in the dataset. We should drop them before proceeding further.

Drop redundant columns
df.drop(['Column1', 'Column2', 'Column3', 'Column4'], axis=1, inplace=True)
Again view summary of dataset
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 7050 entries, 0 to 7049
Data columns (total 12 columns):
status_id           7050 non-null object
status_type         7050 non-null object
status_published    7050 non-null object
num_reactions       7050 non-null int64
num_comments        7050 non-null int64
num_shares          7050 non-null int64
num_likes           7050 non-null int64
num_loves           7050 non-null int64
num_wows            7050 non-null int64
num_hahas           7050 non-null int64
num_sads            7050 non-null int64
num_angrys          7050 non-null int64
dtypes: int64(9), object(3)
memory usage: 661.1+ KB
Now, we can see that redundant columns have been removed from the dataset.

We can see that, there are 3 character variables (data type = object) and remaining 9 numerical variables (data type = int64).

View the statistical summary of numerical variables
df.describe()
num_reactions	num_comments	num_shares	num_likes	num_loves	num_wows	num_hahas	num_sads	num_angrys
count	7050.000000	7050.000000	7050.000000	7050.000000	7050.000000	7050.000000	7050.000000	7050.000000	7050.000000
mean	230.117163	224.356028	40.022553	215.043121	12.728652	1.289362	0.696454	0.243688	0.113191
std	462.625309	889.636820	131.599965	449.472357	39.972930	8.719650	3.957183	1.597156	0.726812
min	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000
25%	17.000000	0.000000	0.000000	17.000000	0.000000	0.000000	0.000000	0.000000	0.000000
50%	59.500000	4.000000	0.000000	58.000000	0.000000	0.000000	0.000000	0.000000	0.000000
75%	219.000000	23.000000	4.000000	184.750000	3.000000	0.000000	0.000000	0.000000	0.000000
max	4710.000000	20990.000000	3424.000000	4710.000000	657.000000	278.000000	157.000000	51.000000	31.000000
There are 3 categorical variables in the dataset. I will explore them one by one.

Explore status_id variable
# view the labels in the variable

df['status_id'].unique()
array(['246675545449582_1649696485147474',
       '246675545449582_1649426988507757',
       '246675545449582_1648730588577397', ...,
       '1050855161656896_1060126464063099',
       '1050855161656896_1058663487542730',
       '1050855161656896_1050858841656528'], dtype=object)
# view how many different types of variables are there

len(df['status_id'].unique())
6997
We can see that there are 6997 unique labels in the status_id variable. The total number of instances in the dataset is 7050. So, it is approximately a unique identifier for each of the instances. Thus this is not a variable that we can use. Hence, I will drop it.

Explore status_published variable
# view the labels in the variable

df['status_published'].unique()
array(['4/22/2018 6:00', '4/21/2018 22:45', '4/21/2018 6:17', ...,
       '9/21/2016 23:03', '9/20/2016 0:43', '9/10/2016 10:30'],
      dtype=object)
# view how many different types of variables are there

len(df['status_published'].unique())
6913
Again, we can see that there are 6913 unique labels in the status_published variable. The total number of instances in the dataset is 7050. So, it is also a approximately a unique identifier for each of the instances. Thus this is not a variable that we can use. Hence, I will drop it also.

Explore status_type variable
# view the labels in the variable

df['status_type'].unique()
array(['video', 'photo', 'link', 'status'], dtype=object)
# view how many different types of variables are there

len(df['status_type'].unique())
4
We can see that there are 4 categories of labels in the status_type variable.

Drop status_id and status_published variable from the dataset
df.drop(['status_id', 'status_published'], axis=1, inplace=True)
View the summary of dataset again
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 7050 entries, 0 to 7049
Data columns (total 10 columns):
status_type      7050 non-null object
num_reactions    7050 non-null int64
num_comments     7050 non-null int64
num_shares       7050 non-null int64
num_likes        7050 non-null int64
num_loves        7050 non-null int64
num_wows         7050 non-null int64
num_hahas        7050 non-null int64
num_sads         7050 non-null int64
num_angrys       7050 non-null int64
dtypes: int64(9), object(1)
memory usage: 550.9+ KB
Preview the dataset again
df.head()
status_type	num_reactions	num_comments	num_shares	num_likes	num_loves	num_wows	num_hahas	num_sads	num_angrys
0	video	529	512	262	432	92	3	1	1	0
1	photo	150	0	0	150	0	0	0	0	0
2	video	227	236	57	204	21	1	1	0	0
3	photo	111	0	0	111	0	0	0	0	0
4	photo	213	0	0	204	9	0	0	0	0
We can see that there is 1 non-numeric column status_type in the dataset. I will convert it into integer equivalents.
